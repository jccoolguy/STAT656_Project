---
title: "Logistic Regression"
author: "Jack"
date: "2025-04-03"
output: html_document
---

# Loading Packages and Data

```{r loadingPackages}
require(dplyr)
require(caret)
require(pROC)
require(corrplot)
```

Adding non-transformed data:

```{r}
load("numeric_features_retained_no_yeoJ.Rdata")
load("factor_features_retained.Rdata")
```

Since logistic regression will suffer when predictors are highly correlated, we check the correlation plot for numeric variables.

# Dropping observations with Missing Flag

```{r}
numeric_features_retained_no_yeoJ = numeric_features_retained_no_yeoJ |> 
  mutate(rows_to_drop = factor_features_retained$missingFlag) |> 
  filter(rows_to_drop == 0) |> 
  select(-rows_to_drop)
factor_features_retained = factor_features_retained |> 
  filter(missingFlag == 0 ) |> 
  select(-c(missingFlag,initial_list_status))
```

# Correlation Filtering

```{r}
NumericVariablesCorr = cor(numeric_features_retained_no_yeoJ)
corrplot(NumericVariablesCorr, order = "hclust",tl.cex = .7)
```

There is a large pocket of correlation, mainly relating to the size of the loan amount.

Since there is evidence of high correlation between features, I decided to go with a correlation filter.

```{r}
highCorr = findCorrelation(NumericVariablesCorr, .8, verbose = TRUE, names = TRUE)
```

```{r}
highCorr = findCorrelation(NumericVariablesCorr, .8, names = TRUE)
```

Filtering out highly correlated features.

```{r}
numeric_features = numeric_features_retained_no_yeoJ[,!colnames(numeric_features_retained_no_yeoJ) %in% highCorr]
```

# Qualitative Feature Analysis and Data Splitting

First I take a look at the frequency of the no credit requirement feature:

```{r}
factor_features_retained |> group_by(nocreditrequirement) |> summarise(n())
```

We can see that it is rare that there is no credit requirement, as such I partition using it so we have enough samples in both the training and test sets.

```{r}
loan_data          = cbind(factor_features_retained, numeric_features)
Y                  = factor_features_retained$loan_status
set.seed(10)
trainingDataIndex  = createDataPartition(loan_data$nocreditrequirement, 
                                         p = .5, list = FALSE)
trainingData       = loan_data[trainingDataIndex,]
testingData        = loan_data[-trainingDataIndex,]
```

# Supervisor

```{r}
Ytrain = factor(select(trainingData, loan_status) |> unlist())
Ytest  = factor(select(testingData,  loan_status) |> unlist())
```

Ensuring default is the event:

```{r}
YtrainRelevel = relevel(Ytrain, ref = 'Charged Off')
YtestRelevel   = relevel(Ytest,  ref = 'Charged Off')
```

# Dummy Variables

```{r}
x_factors     = subset(factor_features_retained, select = - loan_status)
dummyModel    = dummyVars(~., data = x_factors, fullRank = TRUE)

XtrainFactors = predict(dummyModel, x_factors[trainingDataIndex,])
XtestFactors  = predict(dummyModel, x_factors[-trainingDataIndex,])
```

# Combining Numeric, Factor Types

```{r}
XtrainNumeric = numeric_features[trainingDataIndex,]
XtestNumeric  = numeric_features[-trainingDataIndex,]

XtrainFull    = cbind(XtrainFactors, XtrainNumeric)
XtestFull     = cbind(XtestFactors,  XtestNumeric)
```

# Logistic Regression

```{r}
trControl    = trainControl(method = 'none')
outLogistic  = train(x = XtrainFull, y = YtrainRelevel,
                    method = 'glm', trControl = trControl)
YhatTestProb = predict(outLogistic, XtestFull, type = 'prob')
```

# Calibration Check

```{r}
calibPlot  = calibration(YtestRelevel ~ YhatTestProb$`Charged Off`, cuts = 5)
xyplot(calibPlot)
```

Calibration is good for smaller probability estimates, the reason why it doesn't track at higher probability estimates is because default is a rare response:

```{r}
table(Y)
```

As such the model does not predict the probability of a default to be high often:

```{r}
sum(YhatTestProb$`Charged Off` > .5)
```

# Classifications

```{r}
rocCurve = roc(YtestRelevel, YhatTestProb$`Charged Off`)
plot(rocCurve, legacy.axes = TRUE)
```

The ROC curve doesn't hug the top left too well, this is because this is a difficult classification problem.

From here we need to decide what threshold to use.
